{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-307dcd7c6fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config = {}\n",
    "with open('config.json') as data_file:    \n",
    "    config = json.load(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'twitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-f804f1c08b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# on Twitter's OAuth implementation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mCONSUMER_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'twitter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CONSUMER_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mCONSUMER_SECRET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'twitter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CONSUMER_SECRET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mOAUTH_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'twitter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OAUTH_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'twitter'"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "# XXX: Go to http://dev.twitter.com/apps/new to create an app and get values\n",
    "# for these credentials, which you'll need to provide in place of these\n",
    "# empty string values that are defined as placeholders.\n",
    "# See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "# on Twitter's OAuth implementation.\n",
    "\n",
    "CONSUMER_KEY = config['twitter']['CONSUMER_KEY']\n",
    "CONSUMER_SECRET = config['twitter']['CONSUMER_SECRET']\n",
    "OAUTH_TOKEN = config['twitter']['OAUTH_TOKEN']\n",
    "OAUTH_TOKEN_SECRET = config['twitter']['OAUTH_TOKEN_SECRET']\n",
    "\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)\n",
    "\n",
    "# Nothing to see by displaying twitter_api except that it's now a\n",
    "# defined variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get US and World Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "# http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977\n",
    "NYS_WOE_ID = 2459115\n",
    "TX_WOE_ID = 2347602\n",
    "UK_WOE_ID = 23424975\n",
    "\n",
    "\n",
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "trends = []\n",
    "\n",
    "trends.append(twitter_api.trends.place(_id=US_WOE_ID)) #twitter_api.trends.place(_id=WORLD_WOE_ID)\n",
    "trends.append(twitter_api.trends.place(_id=UK_WOE_ID)) #twitter_api.trends.place(_id=US_WOE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#print json.dumps(trends[0], indent=1)\n",
    "#print\n",
    "#print json.dumps(trends[1], indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trend_set = []\n",
    "\n",
    "trend_set.append(set([trend['name'] \n",
    "                        for trend in trends[0][0]['trends']]))\n",
    "\n",
    "trend_set.append(set([trend['name'] \n",
    "                     for trend in trends[1][0]['trends']]))\n",
    "\n",
    "common_trends = trend_set[0].intersection(trend_set[1])\n",
    "\n",
    "trends_subtract = trend_set[1] - trend_set[0]\n",
    "\n",
    "print (\"Trend 1: \\n\" , trend_set[0])\n",
    "print (\"Trend 2: \\n\" , trend_set[1])\n",
    "\n",
    "print (\"\\n{} common trends: \\n\".format(len(common_trends)), common_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries = list(trend_set[0]) + list(trend_set[1])\n",
    "print (len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import unquote to prevent url encoding errors in next_results\n",
    "#from urllib import unquote\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# XXX: Set this variable to a trending topic, \n",
    "# or anything else for that matter. The example query below\n",
    "# was a trending topic when this content was being developed\n",
    "# and is used throughout the remainder of this chapter.\n",
    "\n",
    "#queries = list(common_trends)\n",
    "\n",
    "\n",
    "\n",
    "count = 500\n",
    "\n",
    "trend_tweets = {}\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
    "\n",
    "for q in queries:\n",
    "    print (\"fetching {}\".format(q))\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "    statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "    numberOfTweets = 100\n",
    "    fetchIterations = int(numberOfTweets/count)\n",
    "\n",
    "    for _ in range(fetchIterations):\n",
    "        print (\"Length of statuses {} for {}\".format(len(statuses), q))\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "\n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])    \n",
    "\n",
    "\n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "\n",
    "    trend_tweets[q] = statuses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "tweetText = trend_tweets[queries[0]][0]['text']\n",
    "print (tweetText)\n",
    "urlRegEx = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "noUrl = re.sub(urlRegEx, '', re.sub('RT @','@', tweetText))\n",
    "print ('nourl', noUrl)\n",
    "\n",
    "print (len(queries), len(trend_tweets.keys()))\n",
    "print (trend_tweets[queries[0]][0]['lang'])\n",
    "print (json.dumps(trend_tweets[queries[0]][0], indent=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print trend_tweets.keys()\n",
    "import itertools\n",
    "\n",
    "concatinatedText = {}\n",
    "noUrlText = {}\n",
    "\n",
    "urlRegEx = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "noUrl = re.sub(urlRegEx, '', tweetText)\n",
    "\n",
    "\n",
    "for trend in trend_tweets.keys():\n",
    "    tweet_list = trend_tweets[trend]\n",
    "    #print tweet_list[0]\n",
    "    #only english tweets\n",
    "    concatinatedText[trend] = ''.join([ status['text'] if status['lang']=='en' else ''\n",
    "                                for status in tweet_list])\n",
    "    \n",
    "    noUrlText[trend] = ''.join([ re.sub(urlRegEx, '', re.sub('RT @','@', re.sub('https[:/]*…', '', status['text']))) if status['lang']=='en' else ''\n",
    "                                for status in tweet_list])\n",
    "\n",
    "    print('==========================')\n",
    "    print('TREND: ', trend)\n",
    "    print('TEXT: ', noUrlText[trend])\n",
    "    \n",
    "\n",
    "print (len(trend_tweets.keys()))\n",
    "print (len(noUrlText.values()))\n",
    "#print (noUrlText.values())\n",
    "#print concatinatedText['#SocialismChecklist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, stop_words = 'english',\\\n",
    "strip_accents = 'unicode', lowercase=True, ngram_range=(1,2),\\\n",
    "norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
    "# X is a sparce martrix. each row containes the tfidfs for a trend.\n",
    "X = vectorizer.fit_transform(noUrlText.values())\n",
    "D = -(X * X.T).todense() # dot products\n",
    "XX = X.todense()\n",
    "\n",
    "\n",
    "ind = np.argsort(D)#[::-1][:4]\n",
    "\n",
    "trendIndx = 1\n",
    "print('comparing ', trends[0][0]['trends'][trendIndx])\n",
    "for indx in np.nditer(ind[trendIndx]):\n",
    "    trendGroup = int(math.floor(indx / 50))\n",
    "    groupIndex = indx % 50\n",
    "    trend =  trends[trendGroup][0]['trends'][groupIndex]\n",
    "    if(trend['tweet_volume']):\n",
    "        print(indx, 'british' if trendGroup==1 else 'US', trend['name'], trend['tweet_volume'])\n",
    "\n",
    "\n",
    "        \n",
    "countVectorizer = CountVectorizer(stop_words = 'english',\\\n",
    "strip_accents = 'unicode', lowercase=True)\n",
    "CX = countVectorizer.fit_transform(noUrlText.values())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "features = vectorizer.get_feature_names()\n",
    "top_n = 100\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "#print '\\n Top {} words normalized by document frequency'.format(top_n)\n",
    "#print top_features\n",
    "#freqs = [(word, X.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "#print sorted (freqs, key = lambda x: -x[1])\n",
    "\n",
    "features = countVectorizer.get_feature_names()\n",
    "top_n = 100\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print ('\\n Top {} features'.format(top_n))\n",
    "print (top_features)\n",
    "\n",
    "freqs = [(word, CX.getcol(idx).sum()) for word, idx in countVectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print ('\\n Top words by count')\n",
    "top_words = sorted (freqs, key = lambda x: -x[1])\n",
    "print (top_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "coords = model.fit_transform(XX) \n",
    "print (coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "vis_x = coords[:, 0]\n",
    "vis_y = coords[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = ['#FF0000'] * 50 + ['#0000FF'] * len(list(trends_subtract))\n",
    "\n",
    "\n",
    "print (len(vis_x), len(color))\n",
    "\n",
    "plt.scatter(vis_x, vis_y, color=color)\n",
    "\n",
    "for i, txt in enumerate(queries):\n",
    "    if i<len(vis_x): plt.annotate(txt, (vis_x[i],vis_y[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15.5, 15.5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
